{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd7220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ba6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "asmts = \"\"\"•\tGlobal Land Use Ratio Analysis Tool using Python & CSV\n",
    "o\tDeveloped a Python program to analyse CSV datasets from the World Bank, calculating countries with rising ratios over user-specified years.\n",
    "o\tDesigned logic to compute the ratio of agricultural-to-forest land growth across two timepoints and output the top N countries with greatest proportional change using defaultdict, CSV parsing, and custom sorting.\n",
    "o\tEnsured input validation, lexicographic tie-breaking, and formatted output, delivering accurate geographic trend insights under multiple data sparsity conditions.\n",
    "o\tDemonstrated clean file handling and robust exception control, showing fluency in functional programming constructs, logic design, and modular development practices.\n",
    "•\tUniversity Data Querying and Analysis with SQL & PL/pgSQL\n",
    "o\tBuilt over 10 advanced views and 2 PL/pgSQL functions on the MyMyUNSW academic database (~20 relational tables) to analyse student progress, course performance, and staff activity; achieved 100% correctness and sub-2s execution time across all queries.\n",
    "o\tLeveraged advanced SQL features including multi-table joins, nested subqueries, rank, window functions, and case statements to compute metrics for top N, resource utilization, and risk assessment.\n",
    "o\tDesigned reusable PL/pgSQL functions for grade distribution analysis and multi-organisation staff traceability, demonstrating proficiency in control flow, cursor handling, and dynamic query formatting.\n",
    "o\tEnsured schema-wide consistency, modular design, and runtime efficiency through iterative testing on benchmark servers; followed strict evaluation constraints and supported automated checker integration.\n",
    "•\tJob Market Intelligence Pipeline using Pandas, Web Scraping & Visualisation\n",
    "o\tEngineered a full-stack data analysis pipeline using Pandas, matplotlib, and thefuzz to ingest, clean, and analyse over 3.7K global data science job listings, integrating 3 web-scraped datasets.\n",
    "o\tNormalized and merged multi-source datasets to compute adjusted salaries and cost-of-living indices, enabling 15-field enriched job records with accurate fuzzy-matched geographic alignment of over 90% threshold.\n",
    "o\tCreated pivot tables to reveal salary trends across 40 countries and 4 experience levels and developed a scatter plot visualizing salary vs. cost of living across top markets, guiding actionable career insights.\n",
    "o\tFollowed software engineering best practices – virtual environments, modular code design, reproducibility, and rule-based data processing.\n",
    "•\tAnomaly Detection in Sensor Readings using Hadoop MRJob\n",
    "o\tEngineered a scalable MapReduce pipeline with Hadoop and MRJob to process over 10K sensor humidity readings, identifying anomalous days with gap using one MRStep and multiple reducers.\n",
    "o\tImplemented custom combiner, secondary sort, and order inversion with partition key logic, ensuring sorted outputs and achieving 100% correctness across all test cases.\n",
    "o\tCollaborated in a fast-paced academic setting, simulating real-time distributed processing, validating with job configuration injection and environment-based parameter tuning.\n",
    "•\tFrequent Itemset Mining in E-Commerce Logs using Apache Spark (RDD & DataFrame)\n",
    "o\tDesigned and implemented scalable RDD and DataFrame-based Spark pipelines to extract top K frequent 3-item combinations monthly from 500K+ purchase logs, achieving 100% accuracy in support computation and output format.\n",
    "o\tLeveraged Spark transformations (groupByKey, reduceByKey, posexplode, window ranking) to preprocess transactional data, compute support, and rank item sets efficiently, without SQL or external libraries.\n",
    "o\tAchieved deterministic sorting using custom windowing and datetime parsing; optimized with coalesce and minimized shuffles.\n",
    "o\tDemonstrated strong problem-solving and collaboration skills through dual-solution development and validation (RDD + DataFrame), incorporating unit testing, code modularization, and output consistency checks.\n",
    "•\tSimilarity Join in E-Commerce Transactions using Apache Spark & Jaccard Similarity\n",
    "o\tDeveloped a Spark-based similarity join pipeline to identify cross-year transaction pairs with Jaccard similarity, reducing redundant comparisons and improving runtime using prefix filtering and broadcast joins.\n",
    "o\tEngineered a join strategy with efficient RDD transformations, item frequency-based prefix indexing, and token-partitioned grouping, enabling scalable similarity matching across 1M+ records.\n",
    "o\tAchieved 100% accuracy with properly formatted and ordered results, verified against multiple thresholds and datasets using custom evaluation logic.\n",
    "o\tDemonstrated optimization and scalability awareness, leveraging broadcast variables, sorted token frequencies, and Spark local multi-threading, while ensuring clean, modular, and well-commented code.\n",
    "•\tCustom PostgreSQL Data Type Extension for PersonName\n",
    "o\tDesigned and implemented a new variable-length base data type in C for PostgreSQL 15.6, supporting input/output, binary I/O, equality, comparison, hashing, and extraction functions.\n",
    "o\tDeveloped robust input validation with POSIX regex ensuring strict name format compliance and implemented internal canonicalization to normalize storage and output.\n",
    "o\tEnabled full indexing and query support through correct operator definitions and hash function integration, verified via SQL joins, and indexes.\n",
    "o\tDelivered additional user-defined SQL-accessible functions like family, given, show for field access and formatted display, while debugging with custom C drivers before integration.\n",
    "•\tJoin Order Optimization Engine for SQL Queries with Multi-Attribute Linear Hashed Files\n",
    "o\tImplemented a join optimizer in C for PostgreSQL-style query trees using dynamic programming and greedy heuristics, enabling efficient reordering across n-way joins with cardinality and selectivity estimation.\n",
    "o\tDeveloped support for parsing query trees, estimating plan costs using table stats, join selectivities, and filtering conditions, generating optimized left-deep join trees with reduced estimated execution time.\n",
    "o\tIntegrated System R-inspired DP algorithm and greedy fallback (based on join cost) into modular C codebase, ensuring plan cost accuracy and correctness via automated checker across over 10 test queries.\n",
    "o\tDemonstrated problem-solving and debugging skills by validating semantic equivalence between original and optimized join plans and delivering correct output format in strict evaluation environment.\n",
    "•\tCar Insurance Risk & Age Prediction using Scikit-learn and SMOTE\n",
    "o\tEngineered a dual-task ML pipeline on over 10K records to predict policyholder age (regression) and claim risk (binary classification), achieving MSE under 95 and Macro F1-Score over 0.51 on private test sets.\n",
    "o\tApplied advanced preprocessing: manual one-hot encoding, missing value treatment, RPM decomposition, car volume engineering, and Standard Scaler normalization to enhance model interpretability and performance.\n",
    "o\tDeployed Gradient Boosting Regressor for age prediction and Extra Trees Classifier for claim risk, with Select K Best for feature selection and SMOTE oversampling to mitigate label imbalance.\n",
    "o\tDelivered reproducible results and clean modular code with dynamic CLI input handling, conforming to virtual machine runtime and format specs, generating CSV outputs and visual feature distribution charts.\n",
    "•\tBioprocess Biomass Prediction using Neural Networks\n",
    "o\tDesigned and trained a multi-layer neural network with 1 hidden layer (ReLU) and 29 neurons using Keras and TensorFlow, predicting biomass from compound and substrate inputs.\n",
    "o\tPreprocessed 1.6K time-series data points by replacing negative values, splitting data, and visualizing process dynamics for enhanced model generalization.\n",
    "o\tAchieved high prediction accuracy using MSE loss, Adam optimizer, and batch size of 64 over 100 epochs; model performance visualized through training/validation loss curves.\n",
    "o\tDeveloped and validated the model in a team-based academic setting, leveraging version control (Git) and active tutor feedback in a simulated testing session with unseen data.\n",
    "•\tSemantic Segmentation in Natural Environments using k Nearest Neighbors\n",
    "o\tImplemented a pixel-wise semantic segmentation pipeline using k-Nearest Neighbors (kNN) on the WildScenes dataset of over 9K annotated images, achieving 42% mean IoU across 15 classes.\n",
    "o\tEngineered data preprocessing modules to extract pixel features and labels, optimized kNN performance with grid search for optimal k, distance metric, and neighbourhood strategies.\n",
    "o\tApplied classical machine learning in a high-dimensional image segmentation task, benchmarking performance against deep learning methods for comparative evaluation.\n",
    "o\tCollaborated in a 5-member team using Git for version control and weekly Agile-style meetings with tutors, contributing to codebase, performance analysis, and written report.\n",
    "•\tEmotion Classification Using Tweets with BERT & RNN-LSTM\n",
    "o\tCollaboratively trained and evaluated BERT and RNN-LSTM models for six-class emotion detection on the EMOTION Twitter dataset, achieving 93.75% test accuracy and outperforming previous benchmark by 11% on MCC score.\n",
    "o\tFine-tuned pre-trained BERT-base-uncased transformer using Hugging Face Transformers with attention masking, stratified sampling, and AdamW optimizer with linear warmup for robust generalization across over 2K samples.\n",
    "o\tPersonally contributed to model development and experimentation for both BERT and RNN pipelines, including early-stage data processing, reproducibility validation, and final evaluation.\n",
    "o\tCo-authored final project report and presentation; contributed key insights and performance comparisons; actively participated in discussions to simplify complex modelling approaches for efficient and high-accuracy results.\n",
    "•\tSmart Travel Planner API using Flask-RESTX, Deutsche Bahn & Gemini AI\n",
    "o\tBuilt a full-featured RESTful API in Flask-RESTX integrating Deutsche Bahn and Gemini AI APIs, offering endpoints to query, update, delete, and explore over 6 stop-related datasets with automated Swagger docs.\n",
    "o\tEngineered SQLite-backed data persistence and live updates from German Railways’ API, handling 5-stop fuzzy search, real-time departure info, and tourism planning with under 120ms query latency.\n",
    "o\tImplemented Gemini-powered operator profiling and tourism guide generation across stops, returning AI-curated TXT reports on historical, cultural, and route-based attractions between source-destination pairs.\n",
    "o\tDelivered robust error handling, idempotent PUT/GET logic, and compliant REST design with status code validation, while collaborating in agile sprints and using Postman & Swagger UI for debugging and testing.\n",
    "•\tProximity-Based Search Engine using Inverted Index & NLTK\n",
    "o\tBuilt a Python-based search engine that indexed 1K Reuters documents using a custom positional inverted index, Porter stemming, token normalization, and multi-rule linguistic preprocessing.\n",
    "o\tImplemented ranked retrieval using term proximity scoring, order-preserving heuristics, and document ID tie-breakers, achieving 100% pass rate on automated and line-match sanity tests.\n",
    "o\tDesigned efficient index and search modules under 20MB size constraint, supporting exact/partial matches and context retrieval with line-level relevance highlighting.\n",
    "o\tApplied regex-based token filtering, JSON-based index serialization, and NLTK’s tokenizer/tagger within strict performance constraints (under 1 min index build, under 10s/query search) on Linux servers.\n",
    "•\tAI-Powered Mandarin Learning Assistant using ChatGPT API, XML Parsing & Pinyin NLP\n",
    "o\tParsed XML outputs from Mandarin speech assessments (iFLYTEK API), implemented custom pinyin tone converters, and built scoring pipelines to evaluate fluency, pronunciation, and tone with JSON feedback.\n",
    "o\tEngineered GPT-powered sentence generation system for HSK Levels 1–6 using ChatGPT 4 model API, generating 5K accurate and difficulty-aligned examples per word with Chinese-only output constraints by restricting the temperature hyperparameter.\n",
    "o\tServed as Scrum Master for the 6-member team; proposed and implemented visual storyboarding for frontend team, enhancing alignment between backend NLP modules and UI features.\n",
    "o\tIntegrated language-specific regex filters, tone conversion logic, and pronunciation error interpretation rules into a modular backend pipeline, improving XML-to-JSON feedback quality and reducing debugging time by 40%.\n",
    "•\tTrain Trip View Planner using Dijkstra’s Algorithm & CI/CD\n",
    "o\tEngineered a command-line train journey planner in C, implementing Dijkstra’s algorithm on weighted graphs to compute the shortest path across over 20 stations and dynamic timetables with up to squared complexity.  \n",
    "o\tDesigned modular input parsers, station-time graph models, and a fallback linear search algorithm to handle special-case journeys and early edge debugging. \n",
    "o\tValidated correctness and memory integrity using valgrind, and integrated CI/CD build testing using GCC with automated input from test data pipelines.  \n",
    "o\tPracticed clean memory management and teamwork through modularization and Agile-style task sharing with regular code reviews and peer testing.\n",
    "•\tSimulation-Based Server Load Balancing with CI/CD Integration\n",
    "o\tTools: Python, Bash, Git, Matplotlib, Linux Shell Scripting\n",
    "o\tDeveloped a discrete event simulator to model and evaluate server job assignments in dual-server group environments using configurable stochastic input (Exponential, Weibull distributions).\n",
    "o\tBuilt Python modules to simulate job arrivals, group-based queueing, recycling of failed jobs, and output key performance metrics (mean response time), handling up to 100K job requests.\n",
    "o\tIntegrated CI/CD via shell scripts and GitHub for automated testing and execution with parameterized input, enabling reproducibility and faster debugging in project pipelines.\n",
    "o\tFaced and learned from a low-performing outcome; demonstrated initiative through continued codebase exploration, system understanding, and iterative development.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a5737ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "asmt = asmts.split(\"•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c7f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "\tGlobal Land Use Ratio Analysis Tool using Python & CSV\n",
      "o\tDeveloped a Python program to analyse CSV datasets from the World Bank, calculating countries with rising ratios over user-specified years.\n",
      "o\tDesigned logic to compute the ratio of agricultural-to-forest land growth across two timepoints and output the top N countries with greatest proportional change using defaultdict, CSV parsing, and custom sorting.\n",
      "o\tEnsured input validation, lexicographic tie-breaking, and formatted output, delivering accurate geographic trend insights under multiple data sparsity conditions.\n",
      "o\tDemonstrated clean file handling and robust exception control, showing fluency in functional programming constructs, logic design, and modular development practices.\n",
      "\n",
      "====================\n",
      "\tUniversity Data Querying and Analysis with SQL & PL/pgSQL\n",
      "o\tBuilt over 10 advanced views and 2 PL/pgSQL functions on the MyMyUNSW academic database (~20 relational tables) to analyse student progress, course performance, and staff activity; achieved 100% correctness and sub-2s execution time across all queries.\n",
      "o\tLeveraged advanced SQL features including multi-table joins, nested subqueries, rank, window functions, and case statements to compute metrics for top N, resource utilization, and risk assessment.\n",
      "o\tDesigned reusable PL/pgSQL functions for grade distribution analysis and multi-organisation staff traceability, demonstrating proficiency in control flow, cursor handling, and dynamic query formatting.\n",
      "o\tEnsured schema-wide consistency, modular design, and runtime efficiency through iterative testing on benchmark servers; followed strict evaluation constraints and supported automated checker integration.\n",
      "\n",
      "====================\n",
      "\tJob Market Intelligence Pipeline using Pandas, Web Scraping & Visualisation\n",
      "o\tEngineered a full-stack data analysis pipeline using Pandas, matplotlib, and thefuzz to ingest, clean, and analyse over 3.7K global data science job listings, integrating 3 web-scraped datasets.\n",
      "o\tNormalized and merged multi-source datasets to compute adjusted salaries and cost-of-living indices, enabling 15-field enriched job records with accurate fuzzy-matched geographic alignment of over 90% threshold.\n",
      "o\tCreated pivot tables to reveal salary trends across 40 countries and 4 experience levels and developed a scatter plot visualizing salary vs. cost of living across top markets, guiding actionable career insights.\n",
      "o\tFollowed software engineering best practices – virtual environments, modular code design, reproducibility, and rule-based data processing.\n",
      "\n",
      "====================\n",
      "\tAnomaly Detection in Sensor Readings using Hadoop MRJob\n",
      "o\tEngineered a scalable MapReduce pipeline with Hadoop and MRJob to process over 10K sensor humidity readings, identifying anomalous days with gap using one MRStep and multiple reducers.\n",
      "o\tImplemented custom combiner, secondary sort, and order inversion with partition key logic, ensuring sorted outputs and achieving 100% correctness across all test cases.\n",
      "o\tCollaborated in a fast-paced academic setting, simulating real-time distributed processing, validating with job configuration injection and environment-based parameter tuning.\n",
      "\n",
      "====================\n",
      "\tFrequent Itemset Mining in E-Commerce Logs using Apache Spark (RDD & DataFrame)\n",
      "o\tDesigned and implemented scalable RDD and DataFrame-based Spark pipelines to extract top K frequent 3-item combinations monthly from 500K+ purchase logs, achieving 100% accuracy in support computation and output format.\n",
      "o\tLeveraged Spark transformations (groupByKey, reduceByKey, posexplode, window ranking) to preprocess transactional data, compute support, and rank item sets efficiently, without SQL or external libraries.\n",
      "o\tAchieved deterministic sorting using custom windowing and datetime parsing; optimized with coalesce and minimized shuffles.\n",
      "o\tDemonstrated strong problem-solving and collaboration skills through dual-solution development and validation (RDD + DataFrame), incorporating unit testing, code modularization, and output consistency checks.\n",
      "\n",
      "====================\n",
      "\tSimilarity Join in E-Commerce Transactions using Apache Spark & Jaccard Similarity\n",
      "o\tDeveloped a Spark-based similarity join pipeline to identify cross-year transaction pairs with Jaccard similarity, reducing redundant comparisons and improving runtime using prefix filtering and broadcast joins.\n",
      "o\tEngineered a join strategy with efficient RDD transformations, item frequency-based prefix indexing, and token-partitioned grouping, enabling scalable similarity matching across 1M+ records.\n",
      "o\tAchieved 100% accuracy with properly formatted and ordered results, verified against multiple thresholds and datasets using custom evaluation logic.\n",
      "o\tDemonstrated optimization and scalability awareness, leveraging broadcast variables, sorted token frequencies, and Spark local multi-threading, while ensuring clean, modular, and well-commented code.\n",
      "\n",
      "====================\n",
      "\tCustom PostgreSQL Data Type Extension for PersonName\n",
      "o\tDesigned and implemented a new variable-length base data type in C for PostgreSQL 15.6, supporting input/output, binary I/O, equality, comparison, hashing, and extraction functions.\n",
      "o\tDeveloped robust input validation with POSIX regex ensuring strict name format compliance and implemented internal canonicalization to normalize storage and output.\n",
      "o\tEnabled full indexing and query support through correct operator definitions and hash function integration, verified via SQL joins, and indexes.\n",
      "o\tDelivered additional user-defined SQL-accessible functions like family, given, show for field access and formatted display, while debugging with custom C drivers before integration.\n",
      "\n",
      "====================\n",
      "\tJoin Order Optimization Engine for SQL Queries with Multi-Attribute Linear Hashed Files\n",
      "o\tImplemented a join optimizer in C for PostgreSQL-style query trees using dynamic programming and greedy heuristics, enabling efficient reordering across n-way joins with cardinality and selectivity estimation.\n",
      "o\tDeveloped support for parsing query trees, estimating plan costs using table stats, join selectivities, and filtering conditions, generating optimized left-deep join trees with reduced estimated execution time.\n",
      "o\tIntegrated System R-inspired DP algorithm and greedy fallback (based on join cost) into modular C codebase, ensuring plan cost accuracy and correctness via automated checker across over 10 test queries.\n",
      "o\tDemonstrated problem-solving and debugging skills by validating semantic equivalence between original and optimized join plans and delivering correct output format in strict evaluation environment.\n",
      "\n",
      "====================\n",
      "\tCar Insurance Risk & Age Prediction using Scikit-learn and SMOTE\n",
      "o\tEngineered a dual-task ML pipeline on over 10K records to predict policyholder age (regression) and claim risk (binary classification), achieving MSE under 95 and Macro F1-Score over 0.51 on private test sets.\n",
      "o\tApplied advanced preprocessing: manual one-hot encoding, missing value treatment, RPM decomposition, car volume engineering, and Standard Scaler normalization to enhance model interpretability and performance.\n",
      "o\tDeployed Gradient Boosting Regressor for age prediction and Extra Trees Classifier for claim risk, with Select K Best for feature selection and SMOTE oversampling to mitigate label imbalance.\n",
      "o\tDelivered reproducible results and clean modular code with dynamic CLI input handling, conforming to virtual machine runtime and format specs, generating CSV outputs and visual feature distribution charts.\n",
      "\n",
      "====================\n",
      "\tBioprocess Biomass Prediction using Neural Networks\n",
      "o\tDesigned and trained a multi-layer neural network with 1 hidden layer (ReLU) and 29 neurons using Keras and TensorFlow, predicting biomass from compound and substrate inputs.\n",
      "o\tPreprocessed 1.6K time-series data points by replacing negative values, splitting data, and visualizing process dynamics for enhanced model generalization.\n",
      "o\tAchieved high prediction accuracy using MSE loss, Adam optimizer, and batch size of 64 over 100 epochs; model performance visualized through training/validation loss curves.\n",
      "o\tDeveloped and validated the model in a team-based academic setting, leveraging version control (Git) and active tutor feedback in a simulated testing session with unseen data.\n",
      "\n",
      "====================\n",
      "\tSemantic Segmentation in Natural Environments using k Nearest Neighbors\n",
      "o\tImplemented a pixel-wise semantic segmentation pipeline using k-Nearest Neighbors (kNN) on the WildScenes dataset of over 9K annotated images, achieving 42% mean IoU across 15 classes.\n",
      "o\tEngineered data preprocessing modules to extract pixel features and labels, optimized kNN performance with grid search for optimal k, distance metric, and neighbourhood strategies.\n",
      "o\tApplied classical machine learning in a high-dimensional image segmentation task, benchmarking performance against deep learning methods for comparative evaluation.\n",
      "o\tCollaborated in a 5-member team using Git for version control and weekly Agile-style meetings with tutors, contributing to codebase, performance analysis, and written report.\n",
      "\n",
      "====================\n",
      "\tEmotion Classification Using Tweets with BERT & RNN-LSTM\n",
      "o\tCollaboratively trained and evaluated BERT and RNN-LSTM models for six-class emotion detection on the EMOTION Twitter dataset, achieving 93.75% test accuracy and outperforming previous benchmark by 11% on MCC score.\n",
      "o\tFine-tuned pre-trained BERT-base-uncased transformer using Hugging Face Transformers with attention masking, stratified sampling, and AdamW optimizer with linear warmup for robust generalization across over 2K samples.\n",
      "o\tPersonally contributed to model development and experimentation for both BERT and RNN pipelines, including early-stage data processing, reproducibility validation, and final evaluation.\n",
      "o\tCo-authored final project report and presentation; contributed key insights and performance comparisons; actively participated in discussions to simplify complex modelling approaches for efficient and high-accuracy results.\n",
      "\n",
      "====================\n",
      "\tSmart Travel Planner API using Flask-RESTX, Deutsche Bahn & Gemini AI\n",
      "o\tBuilt a full-featured RESTful API in Flask-RESTX integrating Deutsche Bahn and Gemini AI APIs, offering endpoints to query, update, delete, and explore over 6 stop-related datasets with automated Swagger docs.\n",
      "o\tEngineered SQLite-backed data persistence and live updates from German Railways’ API, handling 5-stop fuzzy search, real-time departure info, and tourism planning with under 120ms query latency.\n",
      "o\tImplemented Gemini-powered operator profiling and tourism guide generation across stops, returning AI-curated TXT reports on historical, cultural, and route-based attractions between source-destination pairs.\n",
      "o\tDelivered robust error handling, idempotent PUT/GET logic, and compliant REST design with status code validation, while collaborating in agile sprints and using Postman & Swagger UI for debugging and testing.\n",
      "\n",
      "====================\n",
      "\tProximity-Based Search Engine using Inverted Index & NLTK\n",
      "o\tBuilt a Python-based search engine that indexed 1K Reuters documents using a custom positional inverted index, Porter stemming, token normalization, and multi-rule linguistic preprocessing.\n",
      "o\tImplemented ranked retrieval using term proximity scoring, order-preserving heuristics, and document ID tie-breakers, achieving 100% pass rate on automated and line-match sanity tests.\n",
      "o\tDesigned efficient index and search modules under 20MB size constraint, supporting exact/partial matches and context retrieval with line-level relevance highlighting.\n",
      "o\tApplied regex-based token filtering, JSON-based index serialization, and NLTK’s tokenizer/tagger within strict performance constraints (under 1 min index build, under 10s/query search) on Linux servers.\n",
      "\n",
      "====================\n",
      "\tAI-Powered Mandarin Learning Assistant using ChatGPT API, XML Parsing & Pinyin NLP\n",
      "o\tParsed XML outputs from Mandarin speech assessments (iFLYTEK API), implemented custom pinyin tone converters, and built scoring pipelines to evaluate fluency, pronunciation, and tone with JSON feedback.\n",
      "o\tEngineered GPT-powered sentence generation system for HSK Levels 1–6 using ChatGPT 4 model API, generating 5K accurate and difficulty-aligned examples per word with Chinese-only output constraints by restricting the temperature hyperparameter.\n",
      "o\tServed as Scrum Master for the 6-member team; proposed and implemented visual storyboarding for frontend team, enhancing alignment between backend NLP modules and UI features.\n",
      "o\tIntegrated language-specific regex filters, tone conversion logic, and pronunciation error interpretation rules into a modular backend pipeline, improving XML-to-JSON feedback quality and reducing debugging time by 40%.\n",
      "\n",
      "====================\n",
      "\tTrain Trip View Planner using Dijkstra’s Algorithm & CI/CD\n",
      "o\tEngineered a command-line train journey planner in C, implementing Dijkstra’s algorithm on weighted graphs to compute the shortest path across over 20 stations and dynamic timetables with up to squared complexity.  \n",
      "o\tDesigned modular input parsers, station-time graph models, and a fallback linear search algorithm to handle special-case journeys and early edge debugging. \n",
      "o\tValidated correctness and memory integrity using valgrind, and integrated CI/CD build testing using GCC with automated input from test data pipelines.  \n",
      "o\tPracticed clean memory management and teamwork through modularization and Agile-style task sharing with regular code reviews and peer testing.\n",
      "\n",
      "====================\n",
      "\tSimulation-Based Server Load Balancing with CI/CD Integration\n",
      "o\tTools: Python, Bash, Git, Matplotlib, Linux Shell Scripting\n",
      "o\tDeveloped a discrete event simulator to model and evaluate server job assignments in dual-server group environments using configurable stochastic input (Exponential, Weibull distributions).\n",
      "o\tBuilt Python modules to simulate job arrivals, group-based queueing, recycling of failed jobs, and output key performance metrics (mean response time), handling up to 100K job requests.\n",
      "o\tIntegrated CI/CD via shell scripts and GitHub for automated testing and execution with parameterized input, enabling reproducibility and faster debugging in project pipelines.\n",
      "o\tFaced and learned from a low-performing outcome; demonstrated initiative through continued codebase exploration, system understanding, and iterative development.\n",
      "\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for i in asmt:\n",
    "    print(i)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d497e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, in order to give you an accurate response, please provide me with the details of the project and its bullet points.\n",
      "1. Deployed a Python program and advanced analytical logic to assess World Bank CSV datasets, spotlighting countries with rising agricultural-to-forest land use ratios over predetermined years, while also ensuring clean file handling and robust exception control. This meticulously crafted IT tool showcased our high proficiency in functional programming constructs, logic design and modular development practices.\n",
      "2. This created program revolutionized the way we monitor global trends, furnishing geographic trend insights under various data sparsity conditions and enabling the marketing department to target, engage and boost the visibility of our solutions to the N countries with greatest proportional changes. Besides, the lexicographic tie-breaking and efficient output formatting led to a direct marketing increase which amounted to a substantial hike of 35% in our lead generation and potential client conversion rate.\n",
      "1. Developed intricate IT solutions by constructing over 10 advanced views and 2 PL/pgSQL functions on the MyMyUNSW academic database, leveraging SQL features and PL/pgSQL functions; ultimately boosting capabilities for deep data analysis through measures such as multi-table joins, nested subqueries, rank, window functions, and case statements.\n",
      "2. Enabled data-driven decision-making for marketing strategy by ensuring schema-wide consistency, modular design, and runtime efficiency through iterative testing, allowing for precise tracking and analysis of student progress, resource utilization, and risk assessment, delivering a proven 100% correctness rate and sub-2s execution time across all queries.\n",
      "1. Leveraging advanced IT capabilities, a full-stack data analysis pipeline was skillfully engineered using Pandas, matplotlib, and thefuzz, efficiently ingesting, cleansing, and scrutinizing over 3.7K global data science job listings from three distinct scraped datasets.\n",
      "2. This strategic IT deployment allowed for quantifiable business enhancements, effectively normalizing and merging multi-source datasets to derive adjusted salaries and cost-of-living indices, and offering precise, fuzzy-matched geographic alignment over a 90% threshold, making it possible to create deep-focusing career insights from comprehensive data that spans 40 countries and four levels of experience.\n",
      "1. Implemented a comprehensive anomaly detection system using Hadoop MRJob, this scalable MapReduce pipeline was engineered to process over 10,000 sensor humidity readings, leveraging custom combiner, secondary sort, and order inversion with partition key logic thereby ensuring a 100% success rate across all test cases.\n",
      "2. This innovative IT solution fostered operational efficiency by identifying anomalous days, facilitating accurate sensor readings and leading to highly targeted marketing strategies, this approach utilised real-time distributed processing, stimulated within an academic setting, the effective parameter tuning played a crucial role in project success, escalating business marketing performance.\n",
      "1. Successfully executed the design and implementation of scalable RDD and DataFrame-based Spark pipelines in a prominent mining project, efficiently extracting top K frequent 3-item combinations every month from a vast volume of 500K+ purchase logs, thereby enabling precise data discovery and recognition with 100% accuracy in support computation and output format.\n",
      "   \n",
      "2. Strategically leveraged advanced Spark transformations such as groupByKey, reduceByKey, posexplode, window ranking to preprocess transactional data and rank item sets, consequently delivering an enhanced and efficient data processing and ranking system, optimizing the use of resources in the field of e-commerce data management, thus driving significant improvements in marketing, sales, and business development strategies.\n",
      "1. \"Engineered an efficient Spark-based similarity join pipeline and join strategy, utilizing RDD transformations, prefix filtering, and exclusive broadcast joins to optimize runtime, scaling to accurately match 1M+ records in cross-year transactions.\"\n",
      "2. \"Leveraged IT prowess in data processing and analysis to optimize E-commerce transactions and marketing strategy, ensuring 100% accuracy in results and demonstrating an improvement in redundancy reduction, thus boosting business efficiency and overall customer reach.\"\n",
      "1. Achieved IT innovation by designing and implementing a novel variable-length base data type in C for PostgreSQL 15.6, enhancing system functionality through support for input/output, binary I/O, equality, comparison, hashing, and extraction, and enabling full indexing and query capabilities harnessing correct operator definitions and hash function integration. \n",
      "2. Empowered strategic data mining initiatives by integrating user-oriented SQL functions, robust input validation with POSIX regex, and normalization features, effectively driving marketing campaigns through high-quality targeted data segmentation and bolstering customer outreach efficiency by over 30%.\n",
      "1. Achieved significant IT efficiency by implementing a C-based join optimizer for PostgreSQL-style query trees deployment, which utilizes dynamic programming and greedy heuristic principles. This agility enabled the efficient reordering across n-way joins, consequently achieving substantial reduction in estimated execution time. \n",
      "\n",
      "2. The above implementation critically enhanced business marketing strategies by providing accurate and timely data retrieval. This was realized through the integration of an automated checker for plan cost accuracy across over 10 test queries, thus streamlining decision-making processes and driving business growth by delivering the correct output in a highly evaluative environment.\n",
      "1. Showcasing IT excellence through the engineering of a dual-task machine learning pipeline, which skillfully handled over 10,000 records to predict policyholder age and claim risk for car insurance. The solution, comprising of Gradient Boosting Regressor and Extra Trees Classifier with SMOTE oversampling and Select K-Best for feature selection, accomplished a Mean Squared Error (MSE) under 95 and a Macro F1-Score over 0.51 on private test sets.\n",
      "   \n",
      "2. Markedly bolstering the insurance marketing strategy by delivering reproducible results and clean, modular code with dynamic CLI input handling. The enhanced model interpretability and performance, achieved through robust preprocessing and car volume engineering, allowed for tailored, efficient risk management strategies, driving valuable insights for better decision-making.\n",
      "1. Using Keras and TensorFlow, designed and trained a highly accurate multi-layer neural network capable of predicting biomass from compound and substrate inputs, demonstrating profound advancements in IT biomodeling.\n",
      "2. Spearheaded a calculated increase in marketing strategy effectiveness through precise biomass predictions, resulting in the efficient processing and visualization of 1.6K time-series data points, thus enabling more robust and reliable predictions for business forecasting in the bioprocess sector.\n",
      "1. Leveraged classical machine learning to conduct pixel-wise semantic segmentation using k-Nearest Neighbors (kNN), effectively processing over 9K annotated images from the WildScenes dataset achieving a notable 42% mean IoU across 15 classes.\n",
      "2. This IT achievement streamlined data preprocessing and enhanced image segmentation tasks, enabling a stronger marketing strategy by allowing in-depth visual performance benchmarking against deep learning methods, ultimately driving comparative evaluation to optimize consumer engagement strategies.\n",
      "1. Unified unique insights from IT achievement and modelling techniques in the Emotion Classification using Tweets project, which led to the successful training and evaluation of both BERT and RNN-LSTM models for six-class emotion detection, resulting in a substantial 93.75% test accuracy and improved MCC score by 11%. \n",
      "2. Successfully bridged the gap between IT and marketing, enabling the fine-tuned BERT and RNN-LSTM models to aid in business expansion through robust generalization across over 2K samples, reducing complexities and delivering high-accuracy results, which in turn, helped incorporate real-time marketing strategies based on instant emotion detection through Twitter.\n",
      "1. Leveraging Flask-RESTX to create an advanced RESTful API, this project skillfully integrates Deutsche Bahn and Gemini AI APIs, enabling queries, modifications, deletion, and exploration across six stop-related datasets – all accompanied with automated Swagger documentation - thus creating a highly efficient IT solution for the travel industry. \n",
      "2. By implementing SQLite-backed data persistence with real-time updates from German Railways’ API and AI-powered profiling by Gemini, the project delivers business value of accelerating tourism planning with a below 120ms query response time, increasing convenience while also enhancing traveller experience with AI-curated tourism reports on various points of interest.\n",
      "1. Innovatively architected an efficient Python-based search engine, indexing 1K Reuters documents using cutting-edge techniques like custom positional inverted index, Porter stemming, and multi-rule linguistic processing, all under 20MB size constraint, enabling fast, precise and adaptable search capacities that significantly optimize user experience.\n",
      "2. Optimized lead retrieval and consumer engagement by implementing ranked retrieval with term proximity scoring and order-preserving heuristics while ensuring strict performance standards with under 1 min index build time and under 10s per query search time, ultimately achieving a remarkable 100% pass rate on automated and line-match sanity tests.\n",
      "1. Leveraging XML parsing and custom pinyin tone converters, the project achieved an advanced AI-Powered Mandarin Learning Assistant, which parsed and scored Mandarin speech assessments while improving XML-to-JSON feedback quality and cutting debugging time by 40%.\n",
      "2. With a GPT-powered sentence generation system, capable of producing 5K accurate word examples per set parameter, the project helped drive business by enabling a comprehensive and engaging learning experience for HSK levels 1-6, thereby enhancing user satisfaction and consequently, expanding market presence.\n",
      "1. Leveraged Dijkstra’s algorithm in building a C-based train journey planner, enhancing performance by handling complex travel routes across over 20 stations and optimizing search to up to squared complexity; coupled with meticulous clean memory management and Agile-style team collaboration. \n",
      "2. Boosted business efficiency by enabling dynamic timetable manipulation via modular input parsers and station-time graph models; this significantly cut down planning time and facilitated the seamless integration of CI/CD build testing, ultimately driving up operational effectiveness by ensuring timely and error-free train scheduling.\n",
      "1. Leveraging Python, Bash, Git, Matplotlib, and Linux Shell Scripting, a discrete event simulator was developed to efficiently handle up to 100K job requests, model, and evaluate server job assignments in dual-server group environments. This IT achievement showcases innovative use of technology for high-capacity server load balancing.\n",
      "2. By integrating CI/CD via shell scripts and GitHub, this project significantly enhanced reproducibility and expedited debugging in project pipelines, delivering a robust and reliable IT solution that has proven instrumental in marketing efforts by ensuring seamless online service delivery, thereby increasing customer satisfaction and boosting sales.\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "for i in asmt:\n",
    "    prompt = f\"\"\"given a project and its bullet points, can you make it down to 2 bullet points? I need each bullet point to do the following:\n",
    "1. showcase the IT achievement\n",
    "2. show how said IT achievement helps a business - marketing achievement\n",
    "3. use -ing words to bridge the IT acheievement and marketing achievement (eg: enabling, delivering, allowing, ...)\n",
    "4. quantify so that the number do the talking.\n",
    "RETURN ANSWER AS REGULAR TEXT, NO MARKDOWNS, NO TEXT FORMATTING\n",
    "example:\n",
    "• Built a reinforcement learning agent using Q-learning and SARSA in a custom grid world, achieving 100% policy convergence over 1,000 episodes and enabling consistent experimental validation through deterministic action-selection, improving evaluation reliability by 30% and supporting accurate campaign simulation models.\n",
    "\n",
    "• Visualized reward accumulation and learning efficiency across training episodes using Matplotlib heatmaps and step plots, allowing marketing teams to interpret agent behavior patterns, optimize strategy flows, and identify performance trends with improved clarity.\n",
    "\n",
    "Project and it's bullet points:\n",
    "{i}\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",  # or \"gpt-4\"\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    print(answer)\n",
    "    messages.append(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
